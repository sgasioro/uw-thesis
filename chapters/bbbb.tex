\chapter{Search for pair production of Higgs bosons in the \bbbb final state}

This chapter presents two complementary searches for pair production of 
Higgs bosons in the final state. Such searches are separated based on the 
signal models being considered: resonant production, in which a new spin-0 or 
spin-2 particle is produced and decays to two Standard Model Higgs bosons, and 
non-resonant production, which is sensitive to the value of the Higgs self-coupling
$\lambda_{HHH}$. Further information on the theory behind both channels can be 
found in Chapter \todo{Fill in theory chapter.}

While the searches face many similar challenges and procede (in broad strokes) in a very 
similar manner, separate optimizations are performed to maximize the respective sensitivities 
for these two very different sets of signal hypotheses. More particularly, resonant signal 
hypotheses are (1) very peaked in values of the mass of the $HH$ candidate system near the 
value of the resonance mass considered and (2) considered across a very broad range of 
signal mass hypotheses. The resonant searches are therefore split into resolved and boosted 
topologies based on Lorentz boost of the decay products, with the resolved channel as one of the 
primary focuses of this thesis. Further, several analysis design decisions are made to 
allow for sensitivity to a broad range of masses -- in particular, though sensitivity is 
limited at lower values of \mhh relative to other channels \todo{Combination, bbyy} due to 
the challenging background topology, retaining and properly reconstructing these low mass events 
allows the \bbbb channel to retain sensitivity up until the kinematic threshold at \SI{250}{\GeV}.

In contrast, non-resonant signal hypotheses are quite broad in \mhh, and have a much more limited 
mass range, with Standard Model production peaking near \SI{400}{GeV}, and the majority of the analysis 
sensitivity able to be captured with a resolved topology. Even for Beyond the Standard 
Model signal hypotheses, which may have more events at low \mhh, the non-resonant nature of the
production allows the \bbbb channel to retain sensitivity while discarding much of the challenging 
low mass background. Such freedom allows for decisions which focus on improved background modeling 
for the middle to upper $HH$ mass regime, resulting in improved modeling and smaller uncertainties 
than would be obtained with a more generic approach.

Both searches are presented in the following, with emphasis on particular motivations for, and consequences
of, the various design decisions involved for each respective set of signal hypotheses. 

The analyses improve upon previous work \todo{cite} in several notable ways. The resonant search leverages 
a Boosted Decision Tree (BDT) based pairing algorithm, offering improved HH pairing efficiency over a broad 
mass spectrum. The non-resonant adopts a different approach, with a simplified algorithm based on the minimum
angular distance ($\Delta R$) between jets in a Higgs candidate. Such an approach very efficiently discards 
low mass background events, resulting in an easier to estimate background with reduced systematic uncertainties.

A particular contribution of this thesis is the background estimation, which uses a novel, neural network
based approach, offering improved modeling over previous methods, as well as the ability to model correlations 
between observables. While all aspects of the analysis of course contribute to the final result, the author of this 
thesis wishes to emphasize that the background estimate, with the corresponding uncertainties and all 
other associated decisions, really is the core of the $HH\rightarrow \bbbb$ analysis -- the development
of this procedure, and all associated decisions, is similarly the core of this thesis work.

ATLAS has performed a variety of searches in complementary decay channels as well,
notably in the $\bbbar\Wplus\Wminus$~\cite{HIGG-2016-27},
$\bbbar\tautau$~\cite{HIGG-2016-16},
$\Wplus\Wminus\Wplus\Wminus$~\cite{HIGG-2016-24},
$\bbbar\gamma\gamma$~\cite{HIGG-2016-15}, and
$\Wplus\Wminus\gamma\gamma$~\cite{HIGG-2016-20} final states, which were combined
along with $\bbbar\bbbar$ in~\cite{HDBS-2018-58}.

CMS has also performed searches for resonant production of Higgs boson pairs in
the $\bbbar\bbbar$ final state (among others) at
$\rts = \SI{8}{\TeV}$~\cite{CMS-EXO-12-053} and
$\rts = \SI{13}{\TeV}$~\cite{CMS-B2G-17-019}. CMS have also performed a
combination of their searches in the $\bbbar\bbbar$, $\bbbar\tautau$,
$\bbbar\gamma\gamma$, and $\bbbar\VVbos$ channels in~\cite{CMS-HIG-17-030}.

This analysis also benefits from improvements to ATLAS jet reconstruction and
calibration, and flavour tagging~\cite{FTAG-2018-01}. In particular, this
analysis benefits from the introduction of particle flow
jets~\cite{PERF-2015-09}. These make use of tracking information to supplement
calorimeter energy deposits, improving the angular and transverse momentum
resolution of jets by better measuring these quantities for charged particles in
those jets.

The analysis also benefits from the new DL1r ATLAS flavour tagging algorithm.
Whereas the flavour tagging algorithm used in the previous analysis (MV2) used a
boosted decision tree (BDT) to combine the output of various low level
algorithms, DL1r (and the baseline DL1 algorithm) uses a deep neural network to
do this combination. In addition to the low level algorithms used as inputs to
MV2, DL1 includes a variety of additional variables used for $\Pqc$-tagging. DL1r
further incorporates RNNIP, a recurrent neural network designed
to identify \bjets using the impact parameters, kinematics, and quality
information of the tracks in the jets, while also taking into account the
correlations between the track features.

The overall analysis sensitivity further benefits from a factor of
\textasciitilde 4.6 increase in integrated luminosity.

\section{Data and Monte Carlo Simulation}
Both the resonant and non-resonant searches are performed on the full ATLAS Run 2 dataset, consisting of 
$\sqrt{s} = \SI{13}{\TeV}$ proton-proton collision data taken from 2016 to 2018 inclusive. Data taken in 2015 
is not used due to a lack of trigger jet matching
information and \bjet trigger scale factors. The integrated luminosity collected
and usable in this analysis\footnote{\label{foot:lost-lumi}approximately
  \SI{9}{\ifb} of data was collected but could not be used in this analysis due
  to an inefficiency in the \bjet triggers at the start of 2016~\cite{ATL-COM-DAQ-2019-150}} was:
\begin{itemize}
  \item \SI{24.6}{\ifb} in 2016
  \item \SI{43.65}{\ifb} in 2017
  \item \SI{57.7}{\ifb} in 2018
\end{itemize}

This gives a total integrated luminosity of \SI{126}{\ifb}.
This is lower than the \SI{139}{\ifb} ATLAS collected during \RunTwo
\cite{ATLAS-CONF-2019-021} due to the inefficiency described in
footnote~\ref{foot:lost-lumi} as well as the \SI{3.2}{\ifb} 
of 2015 data which is unused due to the trigger scale factor 
issue mentioned above.

In this analysis, Monte Carlo samples are used purely for modelling signal
processes. The background is strongly dominated by events produced by QCD
multijet processes, which are difficult to correctly model in simulation. This
necessitates the use of a data-driven background modelling technique, which is
described in \Sect{\ref{sec:bkgdestimation}}.

The scalar resonance signal model is simulated at leading order in \alphas using
\MADGRAPH\cite{MG5}. Hadronization and parton showering are done using
\HERWIGV{7}~\cite{Herwig7}\cite{HerwigPP} with \textsc{EvtGen}~\cite{EvtGen},
and the nominal PDF is NNPDF 2.3 LO. In practice this is implemented as a two
Higgs doublet model where the new neutral scalar is produced through gluon
fusion and required to decay to a pair of SM Higgs bosons. The heavy scalar is
assigned a width much smaller than detector resolution, and the other 2HDM
particles do not enter the calculation.

Scalar samples are produced at resonance masses between \SIlist{251;900}{\GeV} and the detector
simulation is done using AtlFast-II~\cite{SOFT-2010-01}. In addition the samples at \SI{400}{\GeV}
and \SI{900}{\GeV} are also fully simulated to verify that the use of AtlFast-II
is acceptable. For higher masses, as well as for the boosted analysis, 
samples are produced between \SIlist{1000;5000}{\GeV}, and the detector is fully simulated.
As discussed in Chapter \ref{chap:simulation}, an outstanding issue with AtlFast-II is the 
modeling of jet substructure. While such variables are not used for the resolved analysis,
the boosted analysis begins at $\SI{900}{\GeV}$, motivating the different detector 
simulation in these two regimes.

The spin-2 resonance signal model is also simulated at LO in \alphas using
\MADGRAPH. Hadronization and parton showering are done using
\PYTHIAV{8}~\cite{Pythia} with \textsc{EvtGen}, and the nominal PDF is NNPDF 2.3
LO. In practice this is implemented as a Randall-Sundrum graviton with $c=1.0$.

Spin-2 resonance samples are produced at masses between \SIlist{251;5000}{\GeV}, 
and these samples are all produced with full detector simulation.

For the non-resonant search, samples are produced at values of $\kappa_{\lambda} = 1.0$ and $10.0$, and are simulated
using \POWHEGBOX v2 generator~\cite{Powheg1, Powheg2, Powheg3} at next-to-leading order (NLO), with full 
NLO corrections with finite top mass, using the PDF4LHC~\cite{Butterworth:2015oua} parton distribution 
function (PDF) set. Parton showers and hadronization are simulated with \PYTHIAV{8}.

Alternative ggF samples are simulated at NLO using \POWHEGBOX v2, but instead using \Herwig7~\cite{Herwigpp} 
for parton showering and hadronization. The comparison between these two is used to assess an uncertainty 
on the parton showering.


\section{Triggers and Object Definitions}
\label{sec:trigger}
To maximize analysis sensitivity, a combination of multi-\Pqb-jet triggers is used. Due to the 
use of events with two \Pqb-tagged jets in the background estimate, such triggers have a maximum 
requirement of two \Pqb-tagged jets. For the resonant analysis, a combination of triggers of 
various topologies is used, namely \todo{describe}
\begin{itemize}
	\item 2b + HT
	\item 2b + 2j
	\item 2b + 1j
	\item 1b
\end{itemize}
The non-resonant analysis uses a simplified strategy relying entirely on $2b+1j$ and $2b+2j$
triggers.\todo{why?}

While the use of multiple triggers is beneficial for analysis sensitivity, it comes with some
complications. Namely, a set of scale factors must be assigned to simulated events 
account for trigger inefficiencies in data\todo{check}. Because these scale factors may differ between 
triggers, the use of multiple triggers becomes complicated: an event may pass more than one trigger,
while trigger scale factors are only provided for individual triggers.

To simplify this calculation, a set of hierarchical offline selections is applied, closely 
mimicking the trigger selection. Based on these selections, events are sorted into categories
(\emph{trigger buckets}), after which the decision of a \emph{single trigger} is checked. 

The resonant search applies such categorization in the following way, with 
selections considered in order:
\begin{enumerate}
	\item If the leading jet is $b$-tagged with $p_{T} > \SI{325}{\GeV}$, the event is in the $1b$ trigger category.
	\item Otherwise, if the leading jet is not $b$-tagged, but has $p_{T} > \SI{168.75}{\GeV}$, the event is in the $2b+1j$
	trigger category.
	\item If neither of the first two selections pass, if the scalar sum of jet $p_{T}$s, $H_{T} > \SI{900}{\GeV}$,
	the event falls into the $2b+HT$ trigger category.
	\item Events that do not pass any of the above offline selections are in the $2b+2j$ trigger category.
\end{enumerate}
Corresponding triggers are then checked in each category, and the final set of events consists of those events that 
pass the trigger decision in their respective categories. 

For the resonant search, the $2b+1j$ and $2b+2j$ triggers are the dominant categories, containing roughly 26~\% and
49~\% of spin-2 events, evaluated on MC16d samples with resonance masses between \SIlist{300;1200}{\GeV}. Notably, 
the $1b$ trigger efficiency is largest at high $(>\SI{1}{\TeV})$ resonance masses.

For the non-resonant search, it was noted that the $1b$ trigger has minimal contribution, while the $2b+HT$ events are 
largely captured by the $2b+2j$ trigger. Therefore, for, a simplified scheme is considered, 
with selections:
\begin{enumerate}
	\item If the 1st leading jet has $p_{T} > \SI{170}{\GeV}$ and the 3rd leading jet has $p_{T} > \SI{70}{\GeV}$,
	the event is in the $2b+1j$ trigger category.
	\item Otherwise, the event is in the $2b+2j$ trigger category.
\end{enumerate}


\section{Analysis Selection}
After the trigger selections of Section \ref{sec:trigger}, a variety of selections on the analysis objects
are made, with the goal of (1) reconstructing a $HH$-like topology and (2) suppressing contributions from 
background processes. 

Both analyses begin with a common pre-selection, requiring at least four $R=0.4$ anti-$k_{T}$ jets with 
$|\eta| < 2.5$ and $p_{T} > \SI{40}{\GeV}$. The $|\eta| < 2.5$ requirement is necessary for $b$-tagging
due to the coverage of the ATLAS tracking detector (see Chapter \ref{chap:experiment}) \todo{check}, 
while the $p_{T} > \SI{40}{\GeV}$ requirement is motivated by the trigger thresholds \todo{mention low pT}.
At least two of the jets passing this pre-selection are required to be $b$-tagged, and additional $b$-tagging 
requirements are made to define the following regions:
\begin{itemize}
	\item ``2$b$ Region'': require exactly two $b$-tagged jets, used for background estimation
	\item ``4$b$ Region'': require at least (but possibly more) four $b$-tagged jets, used as a signal
	region for both resonant and non-resonant searches
\end{itemize}

The non-resonant analysis additionally defines two $3b$ regions:
\begin{itemize}
	\item ``3$b$+1 loose Region'': require exactly three $b$-tagged jets which 
	pass the 77~\% b-tagging working point (nominal) and one additional jet that fails the  
	77~\% b-tagging working point but passes the \emph{looser} 85~\% b-tagging working point. Used 
	as a signal region for the non-resonant search.
	\item ``3$b$+1 fail Region'': complement of 3$b$+1 loose. Require exactly three $b$-tagged jets 
	which pass the 77~\% b-tagging working point, but require that none of the remaining jets pass 
	the 85~\% b-tagging working point. Used as a validation region for the non-resonant search.
\end{itemize}
After these requirements, four jets are chosen, ranked first by $b$-tagging requirement and then 
by $p_{T}$ (e.g. for the 2$b$ region, the jets chosen are the two $b$-tagged jets and the two 
highest $p_{T}$ non-tagged jets; for the $4b$ region, the jets are the four highest $p_{T}$ $b$-tagged
jets). To match the topology of a $HH\rightarrow \bbbb$ event, these four jets are then \emph{paired}
into \emph{Higgs candidates}: the four jets are split into two sets of two, and each of these pairs is
used to define a reconstructed object that is a proxy for a Higgs in a $HH$ event. \todo{Should pairAGraph go
somewhere? Maybe a ``future ideas'' chapter}

For four jets there are three possible pairings. For signal events, a correct pairing can be identified
(provided all necessary jets pass pre-selection) using the truth information of the Monte Carlo simulation, 
and such information may be used to design/select an appropriate pairing algorithm. This is only part of 
the story, however. The vast majority of the events in data do \emph{not} include a real $HH$ decay (this is 
a search for a reason!), either because the event originates from a background process (e.g. for $4b$ events), or 
because the selection is not designed to maximize the signal (e.g. $2b$ events). As the pairing is 
part of the selection, it must still be run on such events, such that various algorithms which achieve similar
performance in terms of pairing efficiency may have vastly different impacts in terms of the shape of the background
and the biases inherent in the background estimation procedure. The interplay between these two facets of the 
pairing is an important part of the choices made for this analysis.\todo{compare massplanes}

\subsection{Resonant Pairing Strategy}

For the resonant analysis, a Boosted Decision Tree (BDT) is used for the pairing.
The boosted decision tree is given the total separation between the two jets in
each of the two pairs ($\Delta R_{1}$ and $\Delta R_{2}$), the pseudo-rapidity
separation between the two jets in each pair ($\Delta \eta_{1}$ and
$\Delta \eta_{2}$), and the angular separation between the two jets in each pair
in the $x$~--~$y$ plane ($\Delta \phi_{1}$ and $\Delta \phi_{2}$). The total
separations ($\Delta R$s) are provided in addition to the components in order to
avoid requiring the boosted decision tree to reconstruct these variables in
order to use them. For these variables, pair 1 is the pair with the highest
scalar sum of jet \pt{}s, and pair 2 the other pair.

The boosted decision tree is also parameterized on the di-Higgs mass (\mhh)
by providing this as an additional feature. Since the boosted decision tree is
trained on correct and incorrect pairings in signal events, there will be exactly one
correct pairing and two incorrect pairings in the training set for each \mhh
value present in that set. As a result, this variable cannot, in itself,
distinguish a correct pairing from an incorrect pairing, and therefore the
inclusion of this variable simply serves to parameterize the BDT on
\mhh\footnote{That is, the conditions placed on the other variables by the BDT
	vary with \mhh.}.

The boosted decision tree was trained on one quarter of the total AFII
simulated scalar MC statistics, using the Gradient-based One Side Sampling
(GOSS) algorithm which allows rapid training with very large datasets. A
preselection was applied requiring events to have four jets with a \pt of at
least \SI{35}{\GeV}. Note that this is a looser requirement than the
\SI{40}{\GeV} used in the analysis selection, and is meant to increase the
available statistics for events with low \mhh and to ensure a better
performance as a function of that variable. Events were also required to have
four distinct jets that could be geometrically matched (to within
$\Delta R \leq 0.4$) to the \bquarks. The events used to train the BDT were
not included when the analysis was run on these signal simulations. The
boosted decision tree was constructed with the following hyperparameters:\\
\code{min_data_in_leaf=50,\\
	num_leaves=180,\\
	learning_rate=0.01}

These hyperparameters were optimized using a Bayesian optimization
procedure~\cite{skopt}. Three fold cross-validation was used to perform this
optimization without the need for an additional sample, while avoiding
over-training on signal events.

\subsection{Non-resonant Pairing Strategy}
For the non-resonant analysis, a simpler pairing algorithm is used, which 
proceeds as follows: in a given event, Higgs candidates for each possible pairing
are sorted by the $p_{T}$ of the vector sum of constituent jets. The angular separation,
$\dr$ is computed between jets in the each of the leading Higgs candidates, and the 
pairing with the smallest separation ($\dr_{jj}$) is selected. This method will be 
referred to as $\min{\dr}$ in the following.

The primary motivation for the use of this pairing in the non-resonant search is a 
\emph{smooth mass plane}: by efficiently discarding low mass events, $\min{\dr}$ removes 
the background peak present in the resonant search while maintaining good pairing 
efficiency for the non-resonant signal. This facilitates a background estimate with 
small kinematic bias -- the region in which the background estimate is derived is 
more similar to the signal region. 

Along with discarding low mass background, there is a corresponding loss of low mass signal.
In particular \todo{insert plot}, for higher $\kappa_{\lambda}$ points, in which the 
\todo{check - I think it's triangle diagram?} dominates, there is a significant impact 
on the resulting signal distribution. \todo{put in combination plot?} The $4b$ channel 
has the strongest contribution near the Standard Model, however, and is optimized as such.
Further, because of the large low mass background, the loss of significance is mitigated.
This approach is thus adopted for the non-resonant search.


\section{Background Reduction and Top Veto}

Choosing a pairing of the four b-tagged jets fully defines the di-Higgs candidate system used for each event in the remainder of the analysis chain. A requirement of
$\abs{\Delta\eta_{\higgs\higgs}} < 1.5$ on this di-Higgs candidate system mitigates
QCD multijet background.

\Fig{\ref{fig:deta-hh-plots}} illustrates this variable in the validation region
(see \Sect{\ref{sec:kin-reg-def}}). It demonstrates that this selection rejects
only a small fraction of signal, but a significant fraction of data (which, in
the validation region, is a good approximation of pure background).


In order to mitigate the hadronic \ttbar background, a top veto is then applied,
removing events consistent with a \HepProcess{\Pqt \to \Pqb (\PW \to \Pq_{1} \Paq_{2})}
decay.

The jets in the event are separated into \emph{HC jets} which are
the four jets used to build the Higgs candidates, and \emph{non-HC jets}, the
other jets (passing the \pt and $\abs{\eta}$ requirements) in the event.

\PW candidates are built by forming all possible pairs of all jets in each event.
With $n$ jets, there are $\binom{n}{2}$ such pairs. \Pqt candidates are then built
by pairing each \PW candidate with each HC jet (for $4\binom{n}{2}$ combinations).
Note that all jets in a \Pqt candidate must be distinct (i.e. a HC jet may not be
used both on its own and in a \PW candidate).

With $m_{\Pqt}$ denoting the invariant mass of the \Pqt candidate, and $m_{\PW}$
the invariant mass of the \PW candidate, the quantity
\begin{equation}
	\label{eqn:xwt-def}
	X_{\PW\Pqt} = \sqrt{\qty(\frac{10\qty(m_{\PW} - \SI{80.4}{\GeV})}{m_{\PW}})^{2} + \qty(\frac{10\qty(m_{\Pqt} - \SI{172.5}{\GeV})}{m_{\Pqt}})^{2}}
\end{equation}

is constructed for each combination.

Events are then vetoed if the minimum $X_{\PW\Pqt}$ over all combinations is
less than 1.5.

The same definitions and procedures are used for both the resonant and non-resonant analyses.
However, for the non-resonant search, the top candidates considered for $X_{\PW\Pqt}$ have the 
additional requirement that the jet used for the $b$ is $b$-tagged. While this is identical
to the resonant analysis by definition for $4b$ events, it does change the set of events 
considered in lower tag regions, in particular for the $2b$ events considered in the derivation
of the background estimate. Such a change is found to reduce the impact of background systematics
by increasing $2b$ support in the high $X_{\PW\Pqt}$ kinematic region. \todo{Insert plot}

\section{Kinematic Region Definition}
As has been mentioned, an important piece of the analysis is the plane defined by the 
two Higgs candidate masses (the \emph{Higgs candidate mass plane}). After the selection
described above, a signal region is defined by requiring $X_{\higgs\higgs} < 1.6$, where:
\begin{equation}
	\label{eqn:xhh-def}
	X_{\higgs\higgs} = \sqrt{\qty(\frac{\mh1 - c_1}{0.1\cdot\mh1})^{2} + 
	\qty(\frac{\mh2 - c_2}{0.1\cdot\mh2})^{2}}
\end{equation}
with \mh1, \mh2 the leading and subleading Higgs candidate masses, $c_{1}$ and $c_{2}$ correspond
to the center of the signal region, and the denominator provides a Higgs candidate mass 
dependent resolution of 10~\%. For consistency with the $HH$ decay hypothesis, $c_{1}$ and $c_{2}$
are nominally (\SI{125}{\GeV}, \SI{125}{\GeV}). However, these are allowed to vary due to 
energy loss, with specific values chosen described below. The selection of these values is 
one of several significant differences between the regions defined for the resonant and non-resonant search.
We describe both below.

\subsection{Resonant Kinematic Regions}
For the resonant analysis, the signal region is centered at (\SI{120}{\GeV}, \SI{110}{\GeV}) 
to account for energy loss leading to the Higgs masses being under-reconstructed. \todo{insert signal 
location plot?} Note that leading and subleading Higgs candidates are defined according to the 
\emph{scalar sum} of constituent jet $p_{T}$.

For the background estimation, two regions are defined which are roughly concentric around the 
signal region: a \emph{validation region} which 
consists of those events not in the signal region, but which do pass
\begin{equation}
	\label{eqn:val-reg-def}
	\sqrt{\qty(\mh1 - 1.03 \times \SI{120}{\GeV})^{2} + \qty(\mh2 - 1.03 \times
		\SI{110}{\GeV})^{2}} < \SI{30}{\GeV}
\end{equation}
and a \emph{control region} whcih consists of those events not in the signal or validation
regions, but which do pass
\begin{equation}
	\label{eqn:ctrl-reg-def}
	\sqrt{\qty(\mh1 - 1.05 \times \SI{120}{\GeV})^{2} + \qty(\mh2 - 1.05 \times
		\SI{110}{\GeV})^{2}} < \SI{45}{\GeV}
\end{equation}

For simplicity, the \todo{cite old paper} SR/VR/CR definitions were chosen for the resonant analysis, 
but were found to be close to optimal.

\subsection{Non-resonant Kinematic Regions}
For the non-resonant analysis the signal region is centered at (\SI{124}{\GeV}, \SI{117}{\GeV}),
corresponding to the means of \emph{correctly paired} Standard Model signal events. The shape of 
the signal region (other than this change of center) was found to remain optimal.

For the non-resonant search, leading and subleading Higgs candidates are defined according to the 
\emph{vector sum} of constituent jet $p_{T}$, more closely corresponding to the $1\rightarrow 2$ decay assumption
behind the $\min{\dr}$ pairing algorithm. 

Two areas for improvement were identified in the resonant analysis, which will be discussed in more detail below: 
\emph{signal contamination} of the validation region (which impacts the uncertainty assessed due to extrapolation)
and \emph{large nuisance parameter pulls} on this uncertainty, corresponding to a rough assumption that the 
validation region is closer to the signal region in the mass plane, and so offers a better estimate of the 
signal region.

To mitigate these two issues, a redesign of the control and validation regions was performed for 
the non-resonant analysis. The outer boundary defined by the shifted resonant control region:
\begin{equation}
	\label{eqn:ctrl-reg-def}
	\sqrt{\qty(\mh1 - 1.05 \times \SI{124}{\GeV})^{2} + \qty(\mh2 - 1.05 \times
		\SI{117}{\GeV})^{2}} < \SI{45}{\GeV}
\end{equation}
is kept, rougly corresponding to combining the regions used for the resolved analysis. In order 
to assess the variation of the background estimate, two sets of regions are desired, so this combined 
region is split into \emph{quadrants}, that is, divided into four pieces along axes that intersect 
with the signal region center. To avoid kinematic bias, quadrants on opposite sides of the signal 
region are paired, with these pairs corresponding to the non-resonant control and validation regions.

The particular orientation of the regions is chosen such that region centers align with the leading
and subleading Higgs candidate masses, corresponding to a set of axes rotated at $45\circ$, with the ``top''
and ``bottom'' quadrants together comprising the control region, and the other set (``left'' and ``right'')
the validation region.

This design of regions includes a set of events closer to the signal region in the mass plane, leveraging
the assumption that these events are more similar to signal region events, while also including events further away
from the signal region, mitigating signal contamination. This region selection is found to have good performance
in alternate validation regions. \todo{add more studies probs}

\section{Background Estimation}
After the event selection described in Section \ref{sec:selection} there are
two major backgrounds, QCD and \ttbar, with relative proportions \todo{Fill in 
appropriate ggF and VBF background composition}. Following the approach used for the 
resonant analysis, a fully data-driven estimate is used here. This is warranted due to 
the flexibility of the estimation method, as well as the high relative proportion of 
QCD background, and allows for the use of machine learning methods in the construction of
the background estimate. However, it sacrifices an explicit treatment of the
\ttbar component. Performance of the background estimate on the \ttbar component
is thus checked in Appendix \todo{update ttbar studies/do for VBF}. The approach and
its benefits over other methods are discussed below.

\todo{Do small background checks, text here kept from resonant}
Contributions of single Higgs processes to the background are checked in 
Appendix \ref{app:singleHiggs}. $\PZ\PZ$ and $\higgs\higgs$ backgrounds are checked 
in Appendix \ref{app:minor-bkg}. All are found to be negligible.

The foundation of the background estimate lies in the derivation of a
reweighting function which matches the kinematics of events with exactly two
\btagged jets to those of events with four or more \btagged jets. The
reweighting function and overall normalization are derived in the control
region. Systematic bias associated with extrapolating to the signal region in
the Higgs candidate mass plane is assessed in the validation region. All
derivation is done after the full event selection.

\subsection{The Two Tag Region}

Events in data with exactly two b-tagged jets are used for the data driven 
background estimate. The hypothesis here is that, due to the presence of 
multiple \btagged jets, the kinematics of such events are similar to the 
kinematics of events in higher b-tagged regions (i.e. events with three and 
four \btagged jets, respectively), and any differences can be corrected by a 
reweighting procedure. The region with three \btagged jets is split into 
two $b$-tagging regions, with the 3$b$ + 1 loose region used as an additional signal 
region (see Section \todo{Add ref}). The lower tagged 3$b$ component 
(3$b$ + 1 fail, as described in Section~\ref{3b1f}) is reserved for validation
of the background modelling procedure. Events with fewer than two \btagged jets are not 
used for this analysis, as they are relatively more different from the higher tag regions.

The nominal event selection requires at least four jets in order to form Higgs
candidates. For the four tag region, these are the four highest $p_{T}$ \btagged 
jets. For the three tag regions, these jets are the three \btagged jets, plus the 
highest $p_{T}$ jet satisfying a loosened $b$-tagging requirement. Similarly, and following 
the approach of the resonant analysis, the two tag region uses the two \btagged jets 
and the two highest $p_{T}$ non-tagged jets to form Higgs candidates. Combinatoric bias 
from selection of different numbers of \btagged jets is corrected as a part of the kinematic 
reweighting procedure through the reweighting of the total number of jets in the event. In this way, 
the full event selection may be run on two tagged events. 

\subsection{Kinematic Reweighting}
The set of two tagged data events is the fundamental piece of the data driven
background estimate. However, kinematic differences from the four tag region
exist and must be corrected in order for this estimate to be useful. Binned 
approaches based on ratios of histograms (\todo{Cite either 15+16 or full Run 2 VBF})
have been previously considered, but are limited in their handling of correlations 
between variables and by the ``curse of dimensionality'', i.e. the dataset
becomes sparser and sparser in ``reweighting space'' as the number of dimensions
in which to reweight increases, limiting the number of variables used for reweighting. 
This leads either to an unstable fit result (overfitting with finely grained bins) or a 
lower quality fit result (underfitting with coarse bins).

Note that even machine learning methods such as Boosted Decision Trees (BDTs), may suffer
from this curse of dimensionality, as the depth of each decision tree used is limited by 
the available statistics after each set of corresponding selections (cf. binning in a 
more sophisticated way), limiting the expressivity of the learned reweighting function.

To solve these issues, a neural network based reweighting procedure is used
here, following the success of the method used for the resonant search~\cite{Abbott:2708605}. 
This is a truly multivariate approach, allowing for proper treatment of
variable correlations. It further overcomes the issues associated with binned
approaches by learning the reweighting function directly, allowing for greater
sensitivity to local differences and helping to avoid the curse of
dimensionality.


\subsubsection{Neural Network Reweighting}
Let $p_{4b}(x)$ and $p_{2b}(x)$ be the probability density functions for four and two tag data respectively across some input variables $x$. 
The problem of learning the reweighting function between two and four tag data is then the problem of learning a function $w(x)$ such that
\begin{equation}
p_{2b}(x) \cdot w(x) = p_{4b}(x)
\end{equation}
from which it follows that
\begin{equation}
w(x) = \frac{p_{4b}(x)}{p_{2b}(x)}.
\end{equation}

This falls into the domain of density ratio estimation, for which there are a variety
of approaches. The method considered here is modified from ~\cite{NNloss, NNloss1}, and depends on
a loss function of the form
\begin{equation}
\mathcal{L}(R(x)) = \mathbb{E}_{x\sim p_{2b}}[\sqrt{R(x)}]
+\mathbb{E}_{x\sim p_{4b}}[\frac{1}{\sqrt{R(x)}}].
\end{equation}
where $R(x)$ is some estimator dependent on $x$ and $\mathbb{E}_{x\sim p_{2b}}$ and 
$\mathbb{E}_{x\sim p_{4b}}$ are the expectation values with respect to the 2b and 4b probability 
densities. A neural network trained with such a loss function has the objective of finding
the estimator, $R(x)$, that minimizes this loss. It is straightforward to show
(Appendix \ref{app:nnderiv}) that
\begin{equation}
\arg \min_{R}\mathcal{L}(R(x)) = \frac{p_{4b}(x)}{p_{2b}(x)}
\end{equation}
which is exactly the form of the desired reweighting function.

In practice, to avoid imposing explicit positivity constraints, the substitution
$Q(x) \equiv \log R(x)$ is made. The loss function then takes the equivalent form
\begin{equation}
\mathcal{L}(Q(x)) = \mathbb{E}_{x\sim p_{2b}}[\sqrt{e^{Q(x)}}]
+\mathbb{E}_{x\sim p_{4b}}[\frac{1}{\sqrt{e^{Q(x)}}}],
\end{equation}
with solution
\begin{equation}
\arg \min_{Q}\mathcal{L}(Q(x)) = \log\frac{p_{4b}(x)}{p_{2b}(x)}.
\end{equation}
Taking the exponent then results in the desired reweighting function.

\subsubsection{Variables and Results}
The neural network is trained on a variety of variables sensitive to two vs.
four tag differences. To help bring out these differences, the natural logarithm 
of some of the variables with a large, local change is taken. The 
set of training variables for both ggF and VBF channels is shown in Table \ref{tbl:rw-vars}. 
\todo{Update with final configuration/optimization when ready}
\begin{table}[htbp]
	\centering
	\caption{\label{tbl:rw-vars} Set of input variables used for tthe 2\Pqb to 4\Pqb reweighting in the 
	ggF and VBF channels respectively.}
	\begin{tabular}{|p{7cm}|p{7cm}|}
	\hline
	{\bfseries ggF} & {\bfseries VBF} \\
	\hline
	\begin{enumerate}
		\item $\log(p_T)$ of the 4th leading Higgs candidate jet
		\item $\log(p_T)$ of the 2nd leading Higgs candidate jet
		\item $\log(\Delta R)$ between the closest two Higgs candidate jets
		\item $\log(\Delta R)$ between the other two Higgs candidate jets
		\item Average absolute value of Higgs candidate jet $\eta$
		\item $\log(p_T)$ of the di-Higgs system.
		\item $\Delta R$ between the two Higgs candidates
		\item $\Delta \phi$ between the jets in the leading Higgs candidate
		\item $\Delta \phi$ between the jets in the subleading Higgs candidate
		\item $\log(X_{\PW\Pqt})$, where $X_{\PW\Pqt}$ is the variable used for the top veto
		\item Number of jets in the event.
	\end{enumerate}
	&
	\begin{enumerate}
		\item $\log(p_T)$ of the 4th leading Higgs candidate jet
		\item $\log(p_T)$ of the 2nd leading Higgs candidate jet
		\item $\log(\Delta R)$ between the closest two Higgs candidate jets
		\item $\log(\Delta R)$ between the other two Higgs candidate jets
		\item Average absolute value of Higgs candidate jet $\eta$
		\item $\log(p_T)$ of the di-Higgs system.
		\item $\Delta R$ between the two Higgs candidates
		\item $\Delta \phi$ between the jets in the leading Higgs candidate
		\item $\Delta \phi$ between the jets in the subleading Higgs candidate
		\item $\log(X_{\PW\Pqt})$, where $X_{\PW\Pqt}$ is the variable used for the top veto
		\item Number of jets in the event.	
		\item $m_{jj}$ of the VBF jets
		\item $\Delta \eta$ between the VBF jets
		\item $\log(p_T)$ of 6 jets in the event - 4 HC jets and 2 initial scatter jets
		\item $\log(p_T)$ of the leading VBF jet
		\item $\log(p_T)$ of the sub-leading VBF jet
		\item $\Delta \phi$ between VBF jets
		\item Energy of the leading VBF jet
		\item Energy of the sub-leading VBF jet. 
	\end{enumerate}\\
	\hline
	\end{tabular}
\end{table}

As mentioned, this number of jets variable is used to address the combinatoric 
bias of the two-tag region. The neural network used for the ggF reweighting has three 
densely connected hidden layers of 50 nodes each with ReLU activation functions and a single node 
linear output. The same applies to the neural network used for the VBF reweighting.
These configuration demonstrates good performance in the modelling 
of a variety of relevant variables, including \mhh, when compared to a 
range of networks of similar size.

In practice, a given training of the reweighting neural network is subject to variation
due to training statistics and initial conditions. An uncertainty is assigned to account
for this (Section \ref{sec:systematics}), which relies on training an ensemble of
reweighting networks~\cite{DeepEnsembles}. To increase the stability of the background estimate,
the median of the predicted weight for each event is calculated across the ensemble.
This median is then used as the nominal background estimate. This approach is indeed 
seen to be much more stable and to demonstrate a better overall performance than a 
single arbitrary training. Each ensemble used for this analysis consists of 100 
neural networks, trained as described in Section \ref{sec:systematics}.

The training of the ensemble used for the nominal estimate is done in the kinematic
Control Region. The prediction of these networks in the Signal Region is then used
for the nominal background estimate. In addition, a separate ensemble of networks is 
trained in the Validation Region. The difference between the prediction of the nominal 
estimate and the estimate from the VR derived networks in the Signal Region is used to 
assign a systematic uncertainty associated with extrapolating in the Higgs Candidate
mass plane. Further details on this systematic uncertainty are shown in Section \ref{sec:systematics}.
Note that although the same procedure is used for both Control and Validation Region trained 
networks, only the median estimate from the VR derived reweighting is used for assessing the 
extrapolation uncertainty -- no additional ``uncertainty on the uncertainty'' from 
VR ensemble variation is applied.

Each reweighted estimate is normalized such that the reweighted $2b$ yield matches the $4b$ 
yield in the corresponding training region. Note that this applies to each of the networks 
used in each ensemble, where the normalization factor is also subject to the procedure described 
in Section \ref{sec:systematics}. As the median over these normalized weights is not guaranteed 
to preserve this normalization, a further correction is applied such that the $2b$ yield, after 
the median weights are applied, matches the $4b$ yield in the corresponding training region. As no 
preprocessing is applied to correct for the class imbalance between $2b$ and $4b$ events entering 
the training, this ratio of number of $4b$ events ($n(4b)$) over number of $2b$ events ($n(2b)$) is 
folded into the learned weights. Correspondingly, the set of normalization factors described above is 
near $1$ and the learned weights are centered around $n(4b) / n(2b)$ (roughly 0.01 over the full dataset). 
This normalization procedure applies for all instances of the reweighting (e.g. those used for validations 
in Section \ref{sec:bkgvalidation}), with appropriate substitutions of reweighting origin 
(here $2b$) and reweighting target (here $4b$).

Note that, though there are different trigger and pileup selections during each year, the 
statistically limited VBF channel benefits from combining all years together in performing the 
reweighting. The training input is the combined dataset from the years 2016 - 2018 with all configurations remaining unchanged except for the addition of the year of the given event as an input variable on top of the reweighting variables listed in Table~\ref{tbl:rw-vars}. More detailed studies comparing the nominal split-year training and training on all years together for the VBF reweighting is presented in \todo{ref appendix}. For the ggF channel, a similar approach was explored 
\todo{ref appendix}, but was found to have minimal benefit over reweighting in each year separately. 
As the trigger selections for each year significantly impact the kinematics of each year, 
and thus categorizing by year is expected to reflect groupings of kinematically similar events and 
to provide a meaningful degree of freedom in the signal extraction fit, the split-year approach is
kept by the ggF analysis.

\section{Uncertainties}
A variety of uncertainties are assigned to account for known biases in the
underlying methods, calibrations, and objects used for this analysis. The
largest such uncertainty is associated with the kinematic bias inherent in
deriving the background estimate away from the signal region. However, a
statistical biasing of this same estimate has an effect of a similar magnitude.
Additionally, due to the use of Monte Carlo for signal modelling and \btag
calibration, uncertainties related to mismodellings in simulation must also be
accounted for. These components, and their impact on this analysis, are
described here in detail. Relative magnitudes of the uncertainties for
each year are shown in Tables \ref{tbl:errs18}, \ref{tbl:errs17}, and
\ref{tbl:errs16} along with an estimate of the impact of the statistics of 4b
data in the signal region on the total error. Note that, while the Poisson
error (from 2b data statistics) is negligible relative to the bootstrap error 
in the bulk of the distribution, it becomes relevant in the high \mhh tail. 
The final statistical uncertainty used for the limit setting is therefore the 
sum (in quadrature) of these two components. 

%\foreach \yr in {18, 17, 16}{
%	\input{sections/tables/\tablesversion/\NNTtag-error-table-\yr.tex}
%}

\subsection{Statistical Uncertainties and Bootstrapping}
There are two components to the statistical error for the neural network
background estimate. The first is standard Poisson error, i.e., a given bin,
$i$, in the background histogram has value $n_i = \sum\limits_{j\in i} w_j$,
where $w_j$ is the weight for an event $j$ which falls in bin $i$. Standard
techniques then result in statistical error $\delta n_i =
	\sqrt{\sum\limits_{j\in i} w_j^2}$, which reduces to the familiar $\sqrt{N}$
Poisson error when all $w_j$ are equal to 1.

However, this procedure does not take into account the statistical uncertainty
on the $w_j$ due to the finite training dataset. Due to the large size
difference between the two tag and four tag datasets, it is the statistical
uncertainty due to the four tag training data that dominates that on the
background. A standard method for estimating this uncertainty is the bootstrap
resampling technique~\cite{Bootstrap}. Conceptually, a set of statistically
equivalent sets is constructed by sampling with replacement from the original
training set. The reweighting network is then trained on each of these
separately, resulting in a set of statistically equivalent background estimates.
Each of these sets is below referred to as a replica.

In practice, as the original training set is large, the resampling procedure is
able to be simplified through the relation $\lim\limits_{n\rightarrow \infty}
	\operatorname{Binomial}(n, 1/n) = \operatorname{Poisson}(1)$, which dictates that sampling
with replacement is approximately equivalent to applying a randomly distributed
integer weight to each event, drawn from a Poisson distribution with a mean of
1.

Though the network configuration itself is the same for each bootstrap training, the
network initialization is allowed to vary. It should therefore be noted that the bootstrap
uncertainties implicitly capture the uncertainty due to this variation in addition to
the previously mentioned training set variation.

The variation from this bootstrapping procedure is used to assign a bin-by-bin uncertainty
which is treated as a statistical uncertainty in the fit. Due to practical constraints,
a procedure for approximating the full bootstrap error band is developed which demonstrates
good agreement with the full bootstrap uncertainty. This procedure is described below.

\subsubsection{Calculating the Bootstrap Error Band}
The standard procedure to calculate the bootstrap uncertainty would proceed as 
follows: first, each network trained on each bootstrap replica dataset would be used
to produce a histogram in the variable of interest. This would result in a set of 
replica histograms (e.g. for 100 bootstrap replicas, 100 histograms would be created). 
The nominal estimate would then be the mean of bin values across these replica histograms, 
with errors set by the corresponding standard deviation.

In practice, such an approach is inflexible and demanding both in computation and 
in storage, in so far as we would like to produce histograms in many variables, with a 
variety of different cuts and binnings. This motivates a derivation based on event-level 
quantities. However, due to non-trivial correlations between replica weights, simple linear 
propagation of event weight variation is not correct.

We therefore adopt an approach which has been empirically found to produce results (for this
analysis) in line with those produced by generating all of the histograms, as in the standard 
procedure. This approach is described below. Note that, for robustness to outliers and weight distribution 
asymmetry, the median and interquartile range (IQR) are used for the central value and width 
respectively (as opposed to the mean and standard deviation).

The components involved in the calculation have been mentioned in Section \ref{sec:bkgdestimation} and 
are as follows:
\begin{enumerate}
\item Replica weight ($w_{i}$): weight predicted for a given event by a network trained on replica 
dataset $i$.
\item Replica norm ($\alpha_{i}$): normalization factor for replica $i$. This normalizes the reweighting 
prediction of the network trained on replica dataset $i$ to match the correponding target yield.
\item Median weight ($w_{med}$): median weight for a given event across replica datasets, used for the nominal estimate. Defined (for 100 bootstrap replicas) as
\begin{equation}
w_{med} \equiv \operatorname{median}(\alpha_{1}w_{1}, \ldots \alpha_{100}w_{100})
\end{equation}
\item Normalization correction ($\alpha_{med}$): normalization factor to match the predicted yield of the median weights
($w_{med}$) to the target yield in the training region.
\end{enumerate}

As mentioned in Section \ref{sec:bkgdestimation}, the \emph{nominal estimate} is constructed from the set of
median weights and the normalization correction, i.e. $\alpha_{med}\cdot w_{med}$.

For the bootstrap error band, a ``varied'' histogram is then generated by applying, for each event, a weight
equal to the median weight (with no normalization correction) plus half the interquartile range of 
the replica weights: $w_{varied} = w_{med} + \frac{1}{2}\operatorname{IQR}(w_{1},\ldots, w_{100})$.

This varied histogram is scaled to match the yield of the nominal estimate. To account for variation of the 
nominal estimate yield, a normalization variation is calculated from the interquartile range of the replica norms:
$\frac{1}{2}\operatorname{IQR}(\alpha_{1}, \ldots, \alpha_{100})$. This variation, multiplied into the nominal
estimate, is used to set a baseline for the varied histogram described above.

Denoting $H$(weights) as a histogram constructed from a given set of weights, $Y$(weights) as the predicted yield
for a given set of weights, the final varied histogram is thus:
\begin{equation}
H(w_{med} +\frac{1}{2}\operatorname{IQR}(w_{1},\ldots, w_{100}))\cdot \frac{Y(\alpha_{med}w_{med})}{Y(w_{med} +\frac{1}{2}\operatorname{IQR}(w_{1},\ldots, w_{100}))} + \frac{1}{2}\operatorname{IQR}(\alpha_{1}, \ldots, \alpha_{100})\cdot H(\alpha_{med}w_{med})
\end{equation}

where the first term roughly describes the behaviour of the bootstrap variation across the distribution of 
the variable of interest while the second term describes the normalization variation of the bootstrap replicas.

The difference between the varied histogram and the nominal histogram is then
taken to be the bootstrap statistical uncertainty on the nominal histogram.

Figure \todo{include figure} demonstrates how each of the components described above
contribute to the uncertainty envelope for the 2017 Control Region 
and compares this approximate band to the variation of histograms from individual bootstrap 
estimates. The error band constructed from the above procedure is seen to provide a good 
description of the bootstrap variation.

\subsection{Background Shape Uncertainties}
To account for the systematic bias associated with deriving the reweighting function
in the Control region and extrapolating to the Signal region, an alternative background
model is derived in the Validation region. In contrast to previous work, this is
done on the entire background model, due to the fully data-driven nature of the
background used here. The alternative model and the baseline are consistent with
the observed data in their regions and with each other. Differences between the
alternative and baseline models are used to define a shape uncertainty on the \mhh
spectrum, with a two-sided uncertainty defined by symmetrizing the difference about
the baseline.

This uncertainty is split into two components to allow two independent
variations of the \mhh spectrum: \todo{HT vs quad splitting}

\section{Statistical Analysis}
\section{Results}
