\chapter{Future Ideas for $HH\rightarrow \bbbb$}
\label{chap:future}

The searches presented in this thesis make use of a large suite of sophisticated techniques, selected 
through careful study and validation. During this process, a variety of interesting 
directions for the $HH\rightarrow b\bar{b}b\bar{b}$ analysis were explored by this thesis author, 
in collaboration with a few others\footnote{Notably Nicole Hartman (SLAC), who spearheaded much of the 
development and proof of concept work, in collaboration with Michael Kagan and Rafael Teixeira De Lima.}, 
but were not used due to a variety of constraints. We present two such interesting directions here, 
with the hope of encouraging further exploration of these techniques in future work.

\section{pairAGraph: A New Method for Jet Pairing}
As discussed in Chapter \ref{chap:bbbb}, one of the main problems to solve is the pairing of 
$b$-jets into Higgs candidates. Figure \ref{fig:pairing-massplanes} demonstrates that the choice 
of the pairing method, while important for achieving good reconstruction of signal events, also 
significantly impacts the structure of non-$HH$ events, leading to various biases in the 
background estimate. Evaluation of the pairing method therefore must take both of these factors 
into account. While we have presented some advantages in respective contexts for the pairing 
methods considered here, we of course would like to explore further improvements to this important 
component of the analysis.

To that end, we note that all of the pairing methods considered here share a common feature: 
four jets are selected, and the pairing is some discrimination between the available three pairings 
of these four jets. For the methods used in this analysis, the jet selection proceeds via a 
simple $p_{T}$ ordering, with $b$-tagged jets recieving a higher priority than non-tagged jets.

With the advent of a variety of machine learning methods for dealing with a variable number of 
inputs \todo{cite: RNNs, Deep Sets, GNNs, Transformers}, a natural place to improve on the 
pairing is to consider more than just four jets. The pairing and jet selection is then performed 
simultaneously, allowing for the incorporation of more event information in the pairing decision and 
the incorporation of jet correlation structure in the jet selection.

In practice, the majority of $HH\rightarrow\bbbb$ events have either four or five jets which pass 
the kinematic preselection, and any gain from this additional freedom would come from events with 
greater than or equal to five jets. However, this five jet topology is particularly exciting 
for scenarios such as events with initial state radiation (ISR), in which the $HH->4b$ jets are offset by 
a single jet with $p_{T}$ similar in magnitude to that of the $HH->4b$ system. Such events have explicit 
event level information which is not encoded with the inclusion of only the $HH->4b$ jets, and are 
pathological if the ISR jet happens to pass $b$-tagging requirements.

Additionally, with the use of lower tagged regions for background estimation and alternate 
signal regions, this extra flexibility in jet selection may provide a very useful bias -- 
since the algorithm is trained on signal, the selected jets for the pairing will be the 
most ``4b-like'' jets available in the considered set.

For the studies considered here, a transformer \todo{cite Attention} based architecture is used. This is 
best visualized by considering the event as a graph with jets corresponding to nodes and 
edges corresponding to potential connections -- for this reason, we term this algorithm ``pairAGraph''.
The approach is as follows: each jet, $i$, is represented by some vector of input variables, 
$\vec{x}_i$, in our case the four-vector information, $(p_{T}, \eta, \phi, E)$ of each jet, plus 
information on the $b$-tagging decision. A multi-layer perceptron (MLP) is used to create a latent 
embedding, $\mathbf{h}(\vec{x}_i)$, of this input vector.

To describe the relationship between various jets in the event, we then define a vector $\vec{z}_{i}$ 
for each jet as 
\begin{equation}
\vec{z}_i = \sum\limits_j w_{ij}\mathbf{h}(\vec{x}_j)
\end{equation}
where $j$ runs over all jets in the event (including $i=j$), the $w_{ij}$ can be thought of as 
edge weights, and $\mathbf{h}(\vec{x}_j)$ is the latent embedding for jet $j$ mentioned above.

Within this formula, both $\mathbf{h}$ and the $w_{ij}$ are learnable. To learn an appropriate 
latent mapping and set of edge weights, we define a similarity metric corresponding to each 
possible jet pairing:
\begin{equation}
\vec{z}_{1a}\cdot \vec{z}_{1b} + \vec{z}_{2a}\cdot \vec{z}_{2b}
\end{equation}
where subscripts $1a$ and $1b$ correspond to the two jets in pair 1, $2a$ and $2b$ to 
the jets in pair 2 for a given pairing of four distinct jets.

This similarity metric is calculated for all possible pairings, which are then passed through a 
softmax \todo{cite} activation function, which compresses these scores to between $0$ and $1$ 
with sum of $1$, lending an interpretation as probability of each pairing.

In training, the ground truth pairing is set by \emph{truth matching} jets to the $b$-jets 
in the $HH$ signal simulation -- a jet is considered to match if it is $< 0.3$ in $\Delta R$
away from a $b$-jet in the simulation record. Given this ground truth, a cross-entropy loss \todo{cite}
is used on the softmax outputs, and $w_{ij}$ and $\mathbf{h}$ are updated correspondingly.
Training in such a way corresponds to updating $w_{ij}$ and $\mathbf{h}$ to maximize the similarity 
metric for the correct pairing.

In evaluation, the pairings with a higher score (and therefore higher softmax output) given 
the trained $h$ and $w_{ij}$ therefore correspond to the pairings that are most ``HH-like''. 
The maximum over these scores is therefore the pairing used as the predicted result from 
the algorithm.

Because the majority of $HH\rightarrow\bbbb$ events have either four or five jets, it was 
found to be sufficient to only consider a maximum of 5 jets. Consideration of more is in 
principle possible, but the quickly expanding combinatorics leads to a rapidly more 
difficult problem. The jets considered are the five leading jets in $p_{T}$. Notably, 
this set of jets may include jets which are not $b$-tagged, even for the nominal $4b$ 
region -- therefore events with $4$ $b$-jets are not required to use all of them 
in the construction of Higgs candidates, in contrast to the other algorithms used in this 
thesis.

\section{Background Estimation with Mass Plane Interpolation}
The choice of a pairing algorithm that results in a smooth mass plane (such as $\min{\Delta R}$) 
opens up a variety of options for the background estimation. While the method based on 
reweighting of $2b$ events used for this thesis performs well and has been extensively 
studied and validated, it also relies on several assumptions. In particular, the reweighting is derived 
between e.g., $2b$ and $4b$ events \emph{outside} of the signal region and then applied to $2b$ 
events \emph{inside} the signal region, with the assumption that the $2b$ to $4b$ transfer function 
will be sufficiently similar in both regions of the mass plane. An uncertainty is assigned to 
account for the bias due to this assumption, but the extrapolation in the mass plane is never 
explicitly treated in the nominal estimate. While the approach of reweighting $2b$ events within the 
signal region does have the advantage of incorporating explicit signal region information (that is, 
the $2b$ signal region events), the importance of the extrapolation bias motivates consideration 
of a method that operates within the $4b$ mass plane. This additionally removes the reliance on lower 
$b$-tagging regions, allowing for the use of, e.g. $3b$ triggers, and future-proofing the analysis 
against trigger bandwidth constraints in the low tag regions.

The method considered here relies on the following: for a given vector of 
input variables (event kinematics, etc), $\vec{x}$, the joint probability in the $HH$ mass 
plane may be written as:
\begin{equation}
p(\vec{x}, m_{H1}, m_{H2}) = p(\vec{x}| m_{H1}, m_{H2})p(m_{H1}, m_{H2})
\end{equation}
by the chain rule of probability. This means that the full dynamics of events 
in the $HH$ mass plane may be described by (1) the conditional probability of 
considered variables $\vec{x}$, given values of $m_{H1}$ and $m_{H2}$, and (2) the 
density of the mass plane itself. 

We present here an approach which uses normalizing flows \todo{cite} to model the 
conditional probabilities of events in the mass plane and Gaussian processes to 
model the mass plane density. These models are trained in a region around, but not 
including, the signal region, and the trained models are then used to construct an 
\emph{interpolated} estimate of the signal region kinematics. This approach therefore 
explicitly treats event behavior within the mass plane, avoiding the concerns associated 
with a reweighted estimate. Validation of such a method, as well as assessing of closure and 
biases of the method, may be done in alternate $b$-tagging or kinematic regions, notably the 
now unused $2b$ region, results of which are shown below.

\subsection{Normalizing Flows}
Normalizing flows model observed data $x\in X$, with $x \sim p_{X}$, as the output of an 
invertible, differentiable function $f: X \rightarrow Z$, with $z\in Z$ a latent 
variable with a simple prior probability distribution (often standard normal), $z\sim p_{Z}$. 
From a change of variables, given such a function, we may write
\begin{equation}
p_{X}(x) = p_{Z}(f(x))\qty|\det\qty(\frac{d(f(x))}{dx})|
\end{equation}
where $\qty(\frac{d(f(x))}{dx})$ is the Jacobian of $f$ at $x$.

The problem of normalizing flows then reduces to (1) choosing sets of $f$ which are 
both tractable and sufficiently expressive to describe observed data, and (2) optimizing 
associated sets of functional parameters on observed data via maximum likelihood esitmation 
using the above formula. Sampling from the learned density is done by drawing from the 
latent distribution $z\sim p_Z$ (cf. inverse transform sampling) -- the corresponding 
sample is then $x\sim p_X$ with $x=f^{-1}(z)$.

A standard approach to the definition of these $f$ is as a composition of affine transformations (e.g. 
RealNVP \todo{cite}), 
that is, transformations of the form $\alpha z + \beta$, with $\alpha$ and $\beta$ learnable 
parameter vectors. This can roughly be though of as shifting and squeezing the input
input prior density in order to match the data densitiy. However, this has somewhat limited expressivity, 
for instance in the case of a multi-modal density.

This work thus instead relies on neural spline flows \todo{cite: https://arxiv.org/pdf/1906.04032.pdf} 
in which the functions considered are monotonic rational-quadratic splines, which have an analytic inverse.
A rational quadratic function has the form of a quotient of two quadratic polynomials, namely,
\begin{equation}
f_j(x_i) = \frac{a_{ij} x_i^2 + b_{ij} x_{ij} + c_{ij}}{d_{ij}x_i^2 + e_{ij} x_i + f_{ij}}
\end{equation}
with six associated parameters ($a_{ij}$ through $f_{ij}$) per each piecewise bin $j$ of the spline and 
each input dimension $i$. This is explicitly more flexible and expressive than a simple affine 
transformation, allowing, e.g., the treatment of multi-modality via the piecewise nature of the spline. 

The rational quadratic spline is defined on an set interval. The transformation outside of this interval 
is set to the identity, with these linear tails allowing for unconstrained inputs. The boundaries between 
bins of the spline are set by coordinates scalled \emph{knots}, with $K+1$ knots for $K$ bins -- the two 
endpoints for the spline interval plus the $K-1$ internal boundaries. The derivatives 
at these points are constrained to be positive for the internal knots, and boundary 
derivatives are set to 1 to match the linear tails. 

The bin widths and heights are learnable ($2\cdot K$ parameters) as are the internal knot derivatives
($K-1$ parameters), and these $3K-1$ ouputs of the neural network are sufficient to define a monotonic 
rational-quadratic spline which passes through each knot and has the given derivative value at each knot.

In the context of the $HH\rightarrow 4b$ analysis, a neural spline flow is used to model the four vector 
information of each Higgs candidate, conditional on their respective masses. The resulting flow is 
therefore five dimensional, with inputs $x$ = ($p_{T, H1}$, $p_{T, H2}$, $\eta_{H1}$, $\eta_{H2}$, 
$\Delta\phi_{HH}$), where the ATLAS $\phi$ symmetry has been encdoded by assuming $\phi_{H1} = 0$. 
Conditional variables $m_{H1}$ and $m_{H2}$ are not modeled by the flow, but ``come along for the ride''. 
A standard normal distribution in $5$ dimensions is used for the underlying prior. Modeling of the four 
vectors was chosen in order to reduce bias from modeling $m_{HH}$ directly.

The trained flow model then gives a model for $p(x|m_{H1}, m_{H2})$ which may be sampled from to reconstruct 
distributions of $HH$ kinematics given values of $m_{H1}$ and $m_{H2}$.

\subsection{Gaussian Processes}
The second piece of this background estimate is the modeling of the mass plane density, $p(m_{H1}, m_{H2})$. 
This is done using Gaussian process regression -- note that a similar procedure is used to define a systematic 
in the boosted $4b$ analysis. Generally, Gaussian processes are a collection of random variables in which every 
finite collection of said variables is distributed according to a multivariate normal distribution. For the 
context of Gaussian process regression, what we consider is a Gaussian process over function space, that is, 
for a collection of points, $x_{1}, \ldots, x_{N}$, the space of corresponding function 
values, $(f(x_{1}), \ldots, f(x_{N}))$ is Gaussian process distributed, that is, described by an $N$ dimensional 
normal distribution with mean $\mu$, covariance matrix $\Sigma$.

For a single point, this would correspond to a function space described entirely by a normal distribution, 
with various samples from that distribution yielding various candidate functions. For multiple points, the 
covariance matrix describes the relationship between each pair of points -- correspondingly, it is represented 
via a \emph{kernel function}, $K(x, x')$. As, in practice, $\mu$ may always be set to 0 via a centering of the 
data, the kernel function fully defines the considered family of functions.

The considered family of functions describes a Bayesian \emph{prior} for the data. This prior may be conditioned 
on a set of training data points $(X_{1}, \vec{y}_1)$. This conditional \emph{posterior} may then be used to make 
predictions $\vec{y}_2 = f(X_2)$ at a set of new points $X_2$. Because of the Gaussian process prior assumption, 
$\vec{y}_1$ and $\vec{y}_2$ are assumed to be jointly Gaussian. We may therefore write
\begin{equation}
\begin{pmatrix}\vec{y}_1 \\ \vec{y}_2\end{pmatrix} \sim 
\mathcal{N}\begin{pmatrix}\begin{pmatrix}0\\0 \end{pmatrix}, 
\begin{pmatrix}K(X_1, X_1) & K(X_1, X_2)\\ K(X_1, X_2) & K(X_2, X_2)\end{pmatrix}\end{pmatrix}
\end{equation}
where we have used that the kernel function is symmetric and assumed prior mean 0. 

By standard conditioning properties of Gaussian distributions, 
\begin{equation}
\vec{y}_2 | \vec{y}_1 \sim \mathcal{N}(K(X_2, X_1)K(X_1,X_1)^{-1}\vec{y}_1, K(X_2, X_2)-K(X_2,X_1)K(X_1,X_1)^{-1}K(X_1, X_2))
\end{equation}
and which is the sampling distribution for a Gaussian process given kernel $K$. In practice, the 
mean of this sampling distribution is used as the function estimate, with an uncertainty from the
predicted variance at a given point.

The choice of kernel function has a very strong impact on the fitted curve, and must therefore be chosen to 
express the expected dynamics of the data. A common such choice is a radial basis function (RBF) kernel
